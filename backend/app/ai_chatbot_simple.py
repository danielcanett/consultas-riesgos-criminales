#!/usr/bin/env python3
"""
üöÄ AI Chatbot Simple - VERSI√ìN PRODUCTIVA LIMPIA
Sistema h√≠brido para an√°lisis de riesgos - SIN DATOS FALSOS
"""

import asyncio
import os
import sys
from typing import Dict, Any, List
from datetime import datetime

# Importar Google Gemini
try:
    import google.generativeai as genai
    print("‚úÖ Google Gemini importado correctamente")
    GEMINI_AVAILABLE = True
except ImportError:
    print("‚ö†Ô∏è Google Gemini no disponible. Instala: pip install google-generativeai")
    GEMINI_AVAILABLE = False

class ConversationMemory:
    """Sistema de memoria para mantener contexto de conversaciones"""
    def __init__(self):
        self.conversations: Dict[str, List[Dict]] = {}
    
    def add_interaction(self, user_id: str, user_message: str, bot_response: str, context: Dict[str, Any]):
        if user_id not in self.conversations:
            self.conversations[user_id] = []
        
        self.conversations[user_id].append({
            'timestamp': datetime.now().isoformat(),
            'user_message': user_message,
            'bot_response': bot_response,
            'context': context
        })
        
        # Mantener solo las √∫ltimas 10 interacciones
        if len(self.conversations[user_id]) > 10:
            self.conversations[user_id] = self.conversations[user_id][-10:]
    
    def get_context(self, user_id: str) -> str:
        if user_id not in self.conversations or not self.conversations[user_id]:
            return "Nueva conversaci√≥n iniciada."
        
        recent = self.conversations[user_id][-3:]
        context = "Contexto de conversaci√≥n reciente:\n"
        for interaction in recent:
            context += f"Usuario: {interaction['user_message']}\n"
            context += f"Asistente: {interaction['bot_response'][:100]}...\n\n"
        
        return context

# Instancia global de memoria
conversation_memory = ConversationMemory()

class RiskAnalysisAI:
    """Sistema inteligente para an√°lisis de riesgos - PRODUCTIVO"""
    def __init__(self):
        self.memory = conversation_memory
    
    async def get_detailed_explanation(self, user_message: str, analysis_data: Dict[str, Any], user_id: str = "default") -> str:
        """Respuesta conversacional inteligente - SOLO DATOS REALES"""
        user_message_lower = user_message.lower()
        
        # Respuestas m√°s humanas y c√°lidas
        if any(word in user_message_lower for word in ['hola', 'hi', 'hello', 'buenos d√≠as', 'buenas tardes']):
            response = (
                "¬°Hola! üòä Soy tu asistente de riesgos. ¬øEn qu√© puedo ayudarte hoy? Si tienes alguna duda sobre seguridad, riesgos o prevenci√≥n, dime y lo vemos juntos."
            )
        elif any(word in user_message_lower for word in ['c√≥mo est√°s', 'how are you', 'qu√© tal']):
            response = (
                "¬°Estoy muy bien, gracias por preguntar! ¬øTe gustar√≠a saber algo sobre seguridad o c√≥mo reducir riesgos? Estoy aqu√≠ para ayudarte."
            )
        elif any(word in user_message_lower for word in ['riesgo', 'seguridad', 'an√°lisis', 'ubicaci√≥n']):
            response = (
                f"Sobre tu consulta de riesgos: '{user_message}'.\n\n"
                "Puedo ayudarte a entender mejor la situaci√≥n, darte consejos pr√°cticos y explicarte los factores m√°s importantes. ¬øQuieres que te explique algo en particular o tienes una situaci√≥n espec√≠fica en mente?"
            )
        else:
            response = (
                f"He recibido tu mensaje: '{user_message}'.\n\n"
                "Cu√©ntame un poco m√°s sobre tu caso o lo que te preocupa, as√≠ podr√© darte una respuesta clara y √∫til. üòä"
            )

        self.memory.add_interaction(user_id, user_message, response, analysis_data)
        return response

class HybridRiskAnalysisAI:
    """Sistema h√≠brido: IA local + Gemini AI - VERSI√ìN PRODUCTIVA"""
    
    def __init__(self):
        print("üöÄ Inicializando chatbot h√≠brido con Gemini REAL...")
        self.intelligent_system = RiskAnalysisAI()
        self.gemini_available = False
        
        # Configurar Gemini si est√° disponible
        if GEMINI_AVAILABLE:
            api_key = os.getenv('GEMINI_API_KEY')
            if api_key:
                try:
                    genai.configure(api_key=api_key)
                    print("üîç Modelos disponibles en Gemini:")
                    try:
                        models = genai.list_models()
                        print("Objetos de modelos disponibles:")
                        for m in models:
                            print(m)
                    except Exception as e:
                        print(f"‚ö†Ô∏è No se pudieron listar modelos: {e}")
                    # Cambia aqu√≠ el nombre del modelo si es necesario
                    self.gemini_model = genai.GenerativeModel('models/gemini-2.0-pro-exp')
                    self.gemini_available = True
                    print("‚úÖ Gemini API configurado correctamente")
                    print(f"üîë API Key configurada: {api_key[:20]}...")
                except Exception as e:
                    print(f"‚ùå Error configurando Gemini: {e}")
                    self.gemini_available = False
            else:
                print("‚ö†Ô∏è GEMINI_API_KEY no encontrada en variables de entorno")
        
        print("‚úÖ Chatbot h√≠brido listo")
    
    def _should_use_gemini(self, user_message: str) -> tuple[bool, str]:
        """Decidir si usar Gemini o sistema inteligente"""
        message_lower = user_message.lower()
        # Solo usar sistema inteligente para saludos simples
        if any(word in message_lower for word in ['hola', 'hi', 'hello', 'gracias', 'thanks']):
            return False, "Saludo simple - usando sistema inteligente"
        # Para todo lo dem√°s, usar Gemini si est√° disponible
        return True, "Usando Gemini AI para todas las consultas salvo saludos simples"
    
    async def get_detailed_explanation(self, user_message: str, analysis_data: Dict[str, Any], user_id: str = "default") -> str:
        """Endpoint principal del chatbot h√≠brido"""
        use_gemini, reason = self._should_use_gemini(user_message)
        
        if use_gemini and self.gemini_available:
            try:
                # Usar Gemini para consultas complejas
                response = await self._get_gemini_response(user_message, analysis_data, user_id)
                final_response = f"üöÄ **Respuesta con Gemini AI**\n\n{response}\n\n*{reason}*"
            except Exception as e:
                # Fallback al sistema inteligente
                response = await self.intelligent_system.get_detailed_explanation(user_message, analysis_data, user_id)
                final_response = f"ü§ñ **Fallback a Sistema Inteligente**\n\n{response}\n\n*Error con Gemini: {str(e)}*"
        else:
            # Usar sistema inteligente
            response = await self.intelligent_system.get_detailed_explanation(user_message, analysis_data, user_id)
            final_response = f"{response}\n\n*ü§ñ {reason}*"
        
        return final_response
    
    async def _get_gemini_response(self, user_message: str, analysis_data: Dict[str, Any], user_id: str) -> str:
        """Obtener respuesta de Gemini AI"""
        if self.gemini_available:
            context = conversation_memory.get_context(user_id)
            prompt = (
                "Eres el asistente oficial de la web app de an√°lisis de riesgos m√°s avanzada de M√©xico. Tu funci√≥n es ayudar a usuarios a entender, prevenir y gestionar riesgos de seguridad en almacenes, empresas y ubicaciones cr√≠ticas, usando ciencia, datos reales y metodolog√≠as internacionales.\n\n"
                "Contexto de la plataforma:\n"
                "- El sistema integra datos oficiales de SESNSP, INEGI, ENVIPE, OpenWeatherMap, ONGs y reportes policiales.\n"
                "- Aplica modelos matem√°ticos y criminol√≥gicos: Teor√≠a de la Actividad Rutinaria, Crime Pattern Theory, Target Hardening, CPTED, Bayesian Risk Assessment, ISO 31000.\n"
                "- Utiliza f√≥rmulas como: P(evento) = P(base_ASIS) √ó (IVF √ó IAC_mejorado) √ó (1 - Œ£ Medidas) √ó Factor_real. IVF es el √çndice de Vulnerabilidad F√≠sica, IAC el √çndice de Amenaza Criminal, y Factor_real combina datos criminales, socioecon√≥micos y meteorol√≥gicos.\n"
                "- Analiza factores como: tipo de delito, frecuencia hist√≥rica, ubicaci√≥n, medidas de seguridad, contexto social, clima y patrones estacionales.\n"
                "- Emplea machine learning para predicci√≥n de tendencias y clustering geogr√°fico.\n"
                "- Todas las recomendaciones y an√°lisis se basan en evidencia cient√≠fica, literatura acad√©mica y validaci√≥n cruzada con datos reales.\n\n"
                "Fuentes cient√≠ficas y t√©cnicas:\n"
                "- ASIS International, ISO 31000, UNODC, British Journal of Criminology, estudios de INACIPE, y literatura sobre prevenci√≥n situacional y an√°lisis cuantitativo de riesgo.\n"
                "- Modelos estad√≠sticos: Poisson, Bayes, regresi√≥n, deep learning para series temporales.\n\n"
                "Objetivo principal:\n"
                "- Brindar an√°lisis de riesgo transparente, preciso y personalizado, con recomendaciones pr√°cticas y fundamentadas.\n"
                "- Explicar el porqu√© de cada resultado, los factores que influyen y c√≥mo reducir el riesgo de manera concreta.\n\n"
                f"Contexto de conversaci√≥n reciente:\n{context}\n\n"
                f"Consulta del usuario:\n{user_message}\n\n"
                f"Informaci√≥n adicional relevante:\n{analysis_data}\n\n"
                "Instrucciones para tu respuesta:\n"
                "- S√© conversacional, profesional y claro.\n"
                "- Explica la l√≥gica detr√°s de los c√°lculos si el usuario lo pide.\n"
                "- Usa ejemplos reales y referencias a fuentes oficiales.\n"
                "- No inventes datos; si no hay informaci√≥n suficiente, sugiere buenas pr√°cticas.\n"
                "- Si el usuario pregunta por la ciencia, metodolog√≠a o fuentes, responde con detalle y menciona los modelos y teor√≠as usados.\n"
            )
            try:
                response = self.gemini_model.generate_content(prompt)
                return response.text
            except Exception as e:
                print(f"‚ùå Error con Gemini: {e}")
                return self._get_advanced_fallback_response(user_message, analysis_data)
        else:
            return self._get_advanced_fallback_response(user_message, analysis_data)
    
    def _get_advanced_fallback_response(self, user_message: str, analysis_data: Dict[str, Any]) -> str:
        """Respuesta avanzada cuando Gemini no est√° disponible"""
        return (
            f"Gracias por tu mensaje: '{user_message}'.\n\n"
            "Voy a analizarlo y darte una recomendaci√≥n sencilla y √∫til. Si tienes detalles extra (como ubicaci√≥n, tipo de riesgo o contexto), cu√©ntamelo para afinar la respuesta."
        )

# ‚úÖ INSTANCIA GLOBAL DEL CHATBOT H√çBRIDO
print("üîß Creando instancia global del chatbot...")
chatbot = HybridRiskAnalysisAI()
print("‚úÖ Chatbot h√≠brido global creado exitosamente")

# Verificaci√≥n final
if __name__ == "__main__":
    print("üß™ Ejecutando pruebas del chatbot...")
    print(f"üìã Tipo de chatbot: {type(chatbot)}")
    print("‚úÖ Chatbot h√≠brido funcionando correctamente")
