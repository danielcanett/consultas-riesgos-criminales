#!/usr/bin/env python3
"""
ğŸš€ AI Chatbot Simple - VERSIÃ“N PRODUCTIVA LIMPIA
Sistema hÃ­brido para anÃ¡lisis de riesgos - SIN DATOS FALSOS
"""

import asyncio
import os
import sys
from typing import Dict, Any, List
from datetime import datetime

# Importar Google Gemini
try:
    import google.generativeai as genai
    print("âœ… Google Gemini importado correctamente")
    GEMINI_AVAILABLE = True
except ImportError:
    print("âš ï¸ Google Gemini no disponible. Instala: pip install google-generativeai")
    GEMINI_AVAILABLE = False

class ConversationMemory:
    """Sistema de memoria para mantener contexto de conversaciones"""
    def __init__(self):
        self.conversations: Dict[str, List[Dict]] = {}
    
    def add_interaction(self, user_id: str, user_message: str, bot_response: str, context: Dict[str, Any]):
        if user_id not in self.conversations:
            self.conversations[user_id] = []
        
        self.conversations[user_id].append({
            'timestamp': datetime.now().isoformat(),
            'user_message': user_message,
            'bot_response': bot_response,
            'context': context
        })
        
        # Mantener solo las Ãºltimas 10 interacciones
        if len(self.conversations[user_id]) > 10:
            self.conversations[user_id] = self.conversations[user_id][-10:]
    
    def get_context(self, user_id: str) -> str:
        if user_id not in self.conversations or not self.conversations[user_id]:
            return "Nueva conversaciÃ³n iniciada."
        
        recent = self.conversations[user_id][-3:]
        context = "Contexto de conversaciÃ³n reciente:\n"
        for interaction in recent:
            context += f"Usuario: {interaction['user_message']}\n"
            context += f"Asistente: {interaction['bot_response'][:100]}...\n\n"
        
        return context

# Instancia global de memoria
conversation_memory = ConversationMemory()

class RiskAnalysisAI:
    """Sistema inteligente para anÃ¡lisis de riesgos - PRODUCTIVO"""
    def __init__(self):
        self.memory = conversation_memory
    
    async def get_detailed_explanation(self, user_message: str, analysis_data: Dict[str, Any], user_id: str = "default") -> str:
        """Respuesta conversacional inteligente - SOLO DATOS REALES"""
        user_message_lower = user_message.lower()
        
        # Respuestas mÃ¡s humanas y cÃ¡lidas
        if any(word in user_message_lower for word in ['hola', 'hi', 'hello', 'buenos dÃ­as', 'buenas tardes']):
            response = (
                "Â¡Hola! ğŸ˜Š Soy tu asistente de riesgos. Â¿En quÃ© puedo ayudarte hoy? Si tienes alguna duda sobre seguridad, riesgos o prevenciÃ³n, dime y lo vemos juntos."
            )
        elif any(word in user_message_lower for word in ['cÃ³mo estÃ¡s', 'how are you', 'quÃ© tal']):
            response = (
                "Â¡Estoy muy bien, gracias por preguntar! Â¿Te gustarÃ­a saber algo sobre seguridad o cÃ³mo reducir riesgos? Estoy aquÃ­ para ayudarte."
            )
        elif any(word in user_message_lower for word in ['riesgo', 'seguridad', 'anÃ¡lisis', 'ubicaciÃ³n']):
            response = (
                f"Sobre tu consulta de riesgos: '{user_message}'.\n\n"
                "Puedo ayudarte a entender mejor la situaciÃ³n, darte consejos prÃ¡cticos y explicarte los factores mÃ¡s importantes. Â¿Quieres que te explique algo en particular o tienes una situaciÃ³n especÃ­fica en mente?"
            )
        else:
            response = (
                f"He recibido tu mensaje: '{user_message}'.\n\n"
                "CuÃ©ntame un poco mÃ¡s sobre tu caso o lo que te preocupa, asÃ­ podrÃ© darte una respuesta clara y Ãºtil. ğŸ˜Š"
            )

        self.memory.add_interaction(user_id, user_message, response, analysis_data)
        return response

class HybridRiskAnalysisAI:
    """Sistema hÃ­brido: IA local + Gemini AI - VERSIÃ“N PRODUCTIVA"""
    
    def __init__(self):
        print("ğŸš€ Inicializando chatbot hÃ­brido con Gemini REAL...")
        self.intelligent_system = RiskAnalysisAI()
        self.gemini_available = False
        
        # Configurar Gemini si estÃ¡ disponible
        if GEMINI_AVAILABLE:
            api_key = os.getenv('GEMINI_API_KEY')
            if api_key:
                try:
                    genai.configure(api_key=api_key)
                    print("ğŸ” Modelos disponibles en Gemini:")
                    try:
                        models = genai.list_models()
                        print("Objetos de modelos disponibles:")
                        for m in models:
                            print(m)
                    except Exception as e:
                        print(f"âš ï¸ No se pudieron listar modelos: {e}")
                    # Cambia aquÃ­ el nombre del modelo si es necesario
                    self.gemini_model = genai.GenerativeModel('models/gemini-2.0-pro-exp')
                    self.gemini_available = True
                    print("âœ… Gemini API configurado correctamente")
                    print(f"ğŸ”‘ API Key configurada: {api_key[:20]}...")
                except Exception as e:
                    print(f"âŒ Error configurando Gemini: {e}")
                    self.gemini_available = False
            else:
                print("âš ï¸ GEMINI_API_KEY no encontrada en variables de entorno")
        
        print("âœ… Chatbot hÃ­brido listo")
    
    def _should_use_gemini(self, user_message: str) -> tuple[bool, str]:
        """Decidir si usar Gemini o sistema inteligente"""
        message_lower = user_message.lower()
        # Solo usar sistema inteligente para saludos simples
        if any(word in message_lower for word in ['hola', 'hi', 'hello', 'gracias', 'thanks']):
            return False, "Saludo simple - usando sistema inteligente"
        # Para todo lo demÃ¡s, usar Gemini si estÃ¡ disponible
        return True, "Usando Gemini AI para todas las consultas salvo saludos simples"
    
    async def get_detailed_explanation(self, user_message: str, analysis_data: Dict[str, Any], user_id: str = "default") -> str:
        """Endpoint principal del chatbot hÃ­brido"""
        use_gemini, reason = self._should_use_gemini(user_message)
        
        if use_gemini and self.gemini_available:
            try:
                # Usar Gemini para consultas complejas
                response = await self._get_gemini_response(user_message, analysis_data, user_id)
                final_response = f"ğŸš€ **Respuesta con Gemini AI**\n\n{response}\n\n*{reason}*"
            except Exception as e:
                # Fallback al sistema inteligente
                response = await self.intelligent_system.get_detailed_explanation(user_message, analysis_data, user_id)
                final_response = f"ğŸ¤– **Fallback a Sistema Inteligente**\n\n{response}\n\n*Error con Gemini: {str(e)}*"
        else:
            # Usar sistema inteligente
            response = await self.intelligent_system.get_detailed_explanation(user_message, analysis_data, user_id)
            final_response = f"{response}\n\n*ğŸ¤– {reason}*"
        
        return final_response
    
    async def _get_gemini_response(self, user_message: str, analysis_data: Dict[str, Any], user_id: str) -> str:
        """Obtener respuesta de Gemini AI"""
        if self.gemini_available:
            context = conversation_memory.get_context(user_id)
            prompt = (
                "Eres el asistente oficial de la web app de anÃ¡lisis de riesgos mÃ¡s avanzada de MÃ©xico. Tu funciÃ³n es ayudar a usuarios a entender, prevenir y gestionar riesgos de seguridad en almacenes, empresas y ubicaciones crÃ­ticas, usando ciencia, datos reales y metodologÃ­as internacionales.\n\n"
                "Contexto de la plataforma:\n"
                "- El sistema integra datos oficiales de SESNSP, INEGI, ENVIPE, OpenWeatherMap, ONGs y reportes policiales.\n"
                "- Aplica modelos matemÃ¡ticos y criminolÃ³gicos: TeorÃ­a de la Actividad Rutinaria, Crime Pattern Theory, Target Hardening, CPTED, Bayesian Risk Assessment, ISO 31000.\n"
                "- Utiliza fÃ³rmulas como: P(evento) = P(base_ASIS) Ã— (IVF Ã— IAC_mejorado) Ã— (1 - Î£ Medidas) Ã— Factor_real. IVF es el Ãndice de Vulnerabilidad FÃ­sica, IAC el Ãndice de Amenaza Criminal, y Factor_real combina datos criminales, socioeconÃ³micos y meteorolÃ³gicos.\n"
                "- Analiza factores como: tipo de delito, frecuencia histÃ³rica, ubicaciÃ³n, medidas de seguridad, contexto social, clima y patrones estacionales.\n"
                "- Emplea machine learning para predicciÃ³n de tendencias y clustering geogrÃ¡fico.\n"
                "- Todas las recomendaciones y anÃ¡lisis se basan en evidencia cientÃ­fica, literatura acadÃ©mica y validaciÃ³n cruzada con datos reales.\n\n"
                "Fuentes cientÃ­ficas y tÃ©cnicas:\n"
                "- ASIS International, ISO 31000, UNODC, British Journal of Criminology, estudios de INACIPE, y literatura sobre prevenciÃ³n situacional y anÃ¡lisis cuantitativo de riesgo.\n"
                "- Modelos estadÃ­sticos: Poisson, Bayes, regresiÃ³n, deep learning para series temporales.\n\n"
                "Objetivo principal:\n"
                "- Brindar anÃ¡lisis de riesgo transparente, preciso y personalizado, con recomendaciones prÃ¡cticas y fundamentadas.\n"
                "- Explicar el porquÃ© de cada resultado, los factores que influyen y cÃ³mo reducir el riesgo de manera concreta.\n\n"
                f"Contexto de conversaciÃ³n reciente:\n{context}\n\n"
                f"Consulta del usuario:\n{user_message}\n\n"
                f"InformaciÃ³n adicional relevante:\n{analysis_data}\n\n"
                "Instrucciones para tu respuesta:\n"
                "- SÃ© conversacional, profesional y claro.\n"
                "- Explica la lÃ³gica detrÃ¡s de los cÃ¡lculos si el usuario lo pide.\n"
                "- Usa ejemplos reales y referencias a fuentes oficiales.\n"
                "- No inventes datos; si no hay informaciÃ³n suficiente, sugiere buenas prÃ¡cticas.\n"
                "- Si el usuario pregunta por la ciencia, metodologÃ­a o fuentes, responde con detalle y menciona los modelos y teorÃ­as usados.\n"
            )
            try:
                response = self.gemini_model.generate_content(prompt)
                return response.text
            except Exception as e:
                print(f"âŒ Error con Gemini: {e}")
                return self._get_advanced_fallback_response(user_message, analysis_data)
        else:
            return self._get_advanced_fallback_response(user_message, analysis_data)
    
    def _get_advanced_fallback_response(self, user_message: str, analysis_data: Dict[str, Any]) -> str:
        """Respuesta avanzada cuando Gemini no estÃ¡ disponible"""
        return (
            f"Gracias por tu mensaje: '{user_message}'.\n\n"
            "Voy a analizarlo y darte una recomendaciÃ³n sencilla y Ãºtil. Si tienes detalles extra (como ubicaciÃ³n, tipo de riesgo o contexto), cuÃ©ntamelo para afinar la respuesta."
        )

# âœ… INSTANCIA GLOBAL DEL CHATBOT HÃBRIDO
print("ğŸ”§ Creando instancia global del chatbot...")
chatbot = HybridRiskAnalysisAI()
print("âœ… Chatbot hÃ­brido global creado exitosamente")

# VerificaciÃ³n final
if __name__ == "__main__":
    print("ğŸ§ª Ejecutando pruebas del chatbot...")
    print(f"ğŸ“‹ Tipo de chatbot: {type(chatbot)}")
    print("âœ… Chatbot hÃ­brido funcionando correctamente")
